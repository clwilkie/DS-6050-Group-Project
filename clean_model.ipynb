{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25df87da-1207-4692-927f-6816fdbd4814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.26.4\n",
      "TensorFlow version: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "# This is just to make sure your machine has the same versions as mine. if it matches, you can delete this.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5182025c-6839-430e-93fd-8a920cfc4de7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/rebeccavannostrand/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rebeccavannostrand/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/var/folders/dt/bk1f58k14m3122wwk2r1x07c0000gn/T/ipykernel_71769/770582460.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  multi_label_data.loc[:, 'tokens'] = multi_label_data['text'].apply(self.tokenizer.tokenize)\n",
      "/Users/rebeccavannostrand/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeccavannostrand/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.0208 - loss: 0.8699 - val_accuracy: 0.0276 - val_loss: 0.6954 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0557 - loss: 0.8317 - val_accuracy: 0.0491 - val_loss: 0.6944 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.1000 - loss: 0.8071 - val_accuracy: 0.0828 - val_loss: 0.6932 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.1324 - loss: 0.7881 - val_accuracy: 0.1104 - val_loss: 0.6918 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1855 - loss: 0.7700 - val_accuracy: 0.1258 - val_loss: 0.6898 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2251 - loss: 0.7605 - val_accuracy: 0.1472 - val_loss: 0.6869 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2653 - loss: 0.7467 - val_accuracy: 0.1534 - val_loss: 0.6826 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2660 - loss: 0.7370 - val_accuracy: 0.1963 - val_loss: 0.6794 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3333 - loss: 0.7262 - val_accuracy: 0.2178 - val_loss: 0.6759 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3193 - loss: 0.7163 - val_accuracy: 0.2699 - val_loss: 0.6725 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3307 - loss: 0.7087 - val_accuracy: 0.2730 - val_loss: 0.6673 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3873 - loss: 0.6998 - val_accuracy: 0.2975 - val_loss: 0.6623 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3763 - loss: 0.6934 - val_accuracy: 0.3098 - val_loss: 0.6575 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.3924 - loss: 0.6839 - val_accuracy: 0.3282 - val_loss: 0.6505 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.3805 - loss: 0.6789 - val_accuracy: 0.3497 - val_loss: 0.6444 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.3853 - loss: 0.6675 - val_accuracy: 0.3466 - val_loss: 0.6370 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4209 - loss: 0.6584 - val_accuracy: 0.3620 - val_loss: 0.6297 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.3962 - loss: 0.6469 - val_accuracy: 0.3681 - val_loss: 0.6210 - learning_rate: 1.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4133 - loss: 0.6388 - val_accuracy: 0.3712 - val_loss: 0.6089 - learning_rate: 1.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4003 - loss: 0.6288 - val_accuracy: 0.3773 - val_loss: 0.5959 - learning_rate: 1.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4350 - loss: 0.6193 - val_accuracy: 0.3742 - val_loss: 0.5864 - learning_rate: 1.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4422 - loss: 0.6052 - val_accuracy: 0.3804 - val_loss: 0.5775 - learning_rate: 1.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4318 - loss: 0.5980 - val_accuracy: 0.3804 - val_loss: 0.5638 - learning_rate: 1.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4263 - loss: 0.5824 - val_accuracy: 0.3896 - val_loss: 0.5508 - learning_rate: 1.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4482 - loss: 0.5662 - val_accuracy: 0.3834 - val_loss: 0.5411 - learning_rate: 1.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4438 - loss: 0.5539 - val_accuracy: 0.3865 - val_loss: 0.5254 - learning_rate: 1.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4541 - loss: 0.5407 - val_accuracy: 0.4049 - val_loss: 0.5091 - learning_rate: 1.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4541 - loss: 0.5275 - val_accuracy: 0.4018 - val_loss: 0.4937 - learning_rate: 1.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4311 - loss: 0.5116 - val_accuracy: 0.4080 - val_loss: 0.4785 - learning_rate: 1.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4497 - loss: 0.4952 - val_accuracy: 0.4172 - val_loss: 0.4635 - learning_rate: 1.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4445 - loss: 0.4824 - val_accuracy: 0.4141 - val_loss: 0.4431 - learning_rate: 1.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4311 - loss: 0.4637 - val_accuracy: 0.4080 - val_loss: 0.4272 - learning_rate: 1.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4560 - loss: 0.4479 - val_accuracy: 0.4018 - val_loss: 0.4150 - learning_rate: 1.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4562 - loss: 0.4305 - val_accuracy: 0.3957 - val_loss: 0.4034 - learning_rate: 1.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4488 - loss: 0.4072 - val_accuracy: 0.3926 - val_loss: 0.3829 - learning_rate: 1.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4411 - loss: 0.3943 - val_accuracy: 0.3957 - val_loss: 0.3664 - learning_rate: 1.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4487 - loss: 0.3795 - val_accuracy: 0.3957 - val_loss: 0.3461 - learning_rate: 1.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4381 - loss: 0.3646 - val_accuracy: 0.3988 - val_loss: 0.3330 - learning_rate: 1.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4269 - loss: 0.3489 - val_accuracy: 0.3957 - val_loss: 0.3231 - learning_rate: 1.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4481 - loss: 0.3348 - val_accuracy: 0.3988 - val_loss: 0.3066 - learning_rate: 1.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4397 - loss: 0.3173 - val_accuracy: 0.3988 - val_loss: 0.2912 - learning_rate: 1.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4375 - loss: 0.3046 - val_accuracy: 0.3896 - val_loss: 0.2787 - learning_rate: 1.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4381 - loss: 0.2947 - val_accuracy: 0.3926 - val_loss: 0.2644 - learning_rate: 1.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4279 - loss: 0.2797 - val_accuracy: 0.3926 - val_loss: 0.2496 - learning_rate: 1.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4371 - loss: 0.2673 - val_accuracy: 0.3834 - val_loss: 0.2393 - learning_rate: 1.0000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4305 - loss: 0.2572 - val_accuracy: 0.3865 - val_loss: 0.2265 - learning_rate: 1.0000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4366 - loss: 0.2460 - val_accuracy: 0.3926 - val_loss: 0.2191 - learning_rate: 1.0000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4385 - loss: 0.2291 - val_accuracy: 0.3865 - val_loss: 0.2080 - learning_rate: 1.0000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4276 - loss: 0.2184 - val_accuracy: 0.3926 - val_loss: 0.1996 - learning_rate: 1.0000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4228 - loss: 0.2101 - val_accuracy: 0.3896 - val_loss: 0.1878 - learning_rate: 1.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 - 0s - 3ms/step - accuracy: 0.4337 - loss: 0.1766\n",
      "Test Loss: 0.1766, Test Accuracy: 0.4337\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "            acq       0.94      0.62      0.75        77\n",
      "           alum       0.00      0.00      0.00         8\n",
      "         barley       0.91      0.57      0.70        51\n",
      "            bop       0.86      0.82      0.84        74\n",
      "        carcass       0.98      0.89      0.94        57\n",
      "     castor-oil       0.50      0.50      0.50         2\n",
      "          cocoa       1.00      0.75      0.86        12\n",
      "        coconut       1.00      0.20      0.33         5\n",
      "    coconut-oil       0.71      0.71      0.71         7\n",
      "         coffee       1.00      0.67      0.80        27\n",
      "         copper       0.93      0.62      0.74        21\n",
      "     copra-cake       0.33      0.67      0.44         3\n",
      "           corn       0.95      0.96      0.96       237\n",
      "         cotton       0.87      0.57      0.69        35\n",
      "     cotton-oil       0.33      0.33      0.33         3\n",
      "            cpi       0.85      0.85      0.85        26\n",
      "          crude       0.94      0.95      0.94       204\n",
      "            dfl       0.67      0.67      0.67         3\n",
      "            dlr       0.82      0.95      0.88       169\n",
      "            dmk       0.75      0.46      0.57        13\n",
      "           earn       1.00      0.34      0.51        41\n",
      "           fuel       0.64      0.58      0.61        12\n",
      "            gas       0.93      0.72      0.81        36\n",
      "            gnp       0.92      0.98      0.95        62\n",
      "           gold       1.00      0.74      0.85        34\n",
      "          grain       0.97      0.97      0.97       531\n",
      "      groundnut       0.50      0.29      0.36         7\n",
      "  groundnut-oil       0.00      0.00      0.00         2\n",
      "           heat       0.80      0.44      0.57         9\n",
      "            hog       0.93      0.67      0.78        21\n",
      "        housing       1.00      0.67      0.80         3\n",
      "         income       0.75      0.60      0.67         5\n",
      "       interest       0.93      0.96      0.95       206\n",
      "            ipi       0.75      0.75      0.75         8\n",
      "     iron-steel       0.91      0.62      0.74        16\n",
      "            jet       1.00      0.50      0.67         2\n",
      "           jobs       0.87      0.72      0.79        18\n",
      "       l-cattle       1.00      0.86      0.92         7\n",
      "           lead       0.93      0.67      0.78        21\n",
      "            lei       0.00      0.00      0.00         1\n",
      "        lin-oil       0.50      0.50      0.50         2\n",
      "      livestock       0.94      0.88      0.91        77\n",
      "         lumber       1.00      0.50      0.67         2\n",
      "      meal-feed       0.91      0.55      0.69        38\n",
      "       money-fx       0.97      0.97      0.97       408\n",
      "   money-supply       0.71      0.52      0.60        23\n",
      "        naphtha       0.75      0.60      0.67         5\n",
      "        nat-gas       0.95      0.77      0.85        69\n",
      "         nickel       0.00      0.00      0.00         5\n",
      "            nkr       1.00      0.33      0.50         3\n",
      "          nzdlr       1.00      0.33      0.50         3\n",
      "            oat       0.83      0.36      0.50        14\n",
      "        oilseed       0.91      0.83      0.87       162\n",
      "         orange       0.00      0.00      0.00         5\n",
      "      palladium       1.00      0.33      0.50         3\n",
      "       palm-oil       1.00      0.75      0.86        40\n",
      "     palmkernel       1.00      0.67      0.80         3\n",
      "       pet-chem       0.78      0.54      0.64        13\n",
      "       platinum       1.00      0.33      0.50         9\n",
      "         potato       0.00      0.00      0.00         1\n",
      "        propane       0.50      0.20      0.29         5\n",
      "           rand       0.00      0.00      0.00         2\n",
      "       rape-oil       1.00      0.12      0.22         8\n",
      "       rapeseed       1.00      0.41      0.58        27\n",
      "       reserves       0.94      0.67      0.78        24\n",
      "         retail       1.00      0.40      0.57         5\n",
      "           rice       0.86      0.74      0.80        58\n",
      "         rubber       0.75      0.33      0.46         9\n",
      "            rye       0.00      0.00      0.00         2\n",
      "           ship       0.99      0.80      0.88       142\n",
      "         silver       1.00      0.68      0.81        25\n",
      "        sorghum       0.95      0.62      0.75        34\n",
      "       soy-meal       0.94      0.65      0.77        26\n",
      "        soy-oil       0.88      0.56      0.68        25\n",
      "        soybean       0.93      0.82      0.87       111\n",
      "strategic-metal       0.83      0.42      0.56        12\n",
      "          sugar       0.88      0.72      0.79        40\n",
      "       sun-meal       1.00      0.50      0.67         2\n",
      "        sun-oil       0.67      0.29      0.40         7\n",
      "        sunseed       0.91      0.62      0.74        16\n",
      "            tea       1.00      0.62      0.77         8\n",
      "            tin       1.00      0.67      0.80         3\n",
      "          trade       0.94      0.77      0.85       159\n",
      "        veg-oil       0.89      0.60      0.71        94\n",
      "          wheat       0.93      0.97      0.95       283\n",
      "            wpi       1.00      0.17      0.29         6\n",
      "            yen       0.72      0.49      0.58        53\n",
      "           zinc       0.86      0.57      0.69        21\n",
      "\n",
      "      micro avg       0.93      0.82      0.87      4168\n",
      "      macro avg       0.79      0.56      0.63      4168\n",
      "   weighted avg       0.92      0.82      0.86      4168\n",
      "    samples avg       0.88      0.83      0.84      4168\n",
      "\n",
      "\n",
      "Overall Test Accuracy: 0.6437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeccavannostrand/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/rebeccavannostrand/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import reuters, stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import json\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('reuters')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Unified Tokenizer Class\n",
    "class UnifiedTokenizer:\n",
    "    def __init__(self, stop_words, min_length=4, use_stemming=False):\n",
    "        self.stop_words = stop_words\n",
    "        self.min_length = min_length\n",
    "        self.use_stemming = use_stemming\n",
    "        self.stemmer = PorterStemmer() if use_stemming else None\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(f'[{re.escape(string.punctuation)}]', ' ', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        tokens = text.split()\n",
    "        tokens = [\n",
    "            self.stemmer.stem(word) if self.use_stemming else word\n",
    "            for word in tokens\n",
    "            if word not in self.stop_words and len(word) >= self.min_length\n",
    "        ]\n",
    "        return tokens\n",
    "\n",
    "# Reuters Preprocessor Class\n",
    "class ReutersPreprocessor:\n",
    "    def __init__(self, tokenizer, max_features=10000, max_sequence_length=500):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_features = max_features\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.vocabulary = {}\n",
    "\n",
    "    def preprocess(self):\n",
    "        documents = reuters.fileids()\n",
    "        texts = [reuters.raw(doc_id) for doc_id in documents]\n",
    "        labels = [reuters.categories(doc_id) for doc_id in documents]\n",
    "\n",
    "        data = pd.DataFrame({'document_id': documents, 'text': texts, 'labels': labels})\n",
    "        multi_label_data = data[data['labels'].apply(len) > 1]\n",
    "\n",
    "        multi_label_data.loc[:, 'tokens'] = multi_label_data['text'].apply(self.tokenizer.tokenize)\n",
    "\n",
    "        # Create BoW features\n",
    "        vectorizer = CountVectorizer(max_features=self.max_features, tokenizer=lambda x: x, preprocessor=lambda x: x)\n",
    "        X_bow = vectorizer.fit_transform(multi_label_data['tokens']).toarray()\n",
    "\n",
    "        # Perform LDA for topic extraction\n",
    "        lda = LatentDirichletAllocation(n_components=15, random_state=42)\n",
    "        X_lda = lda.fit_transform(X_bow)\n",
    "\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        y = mlb.fit_transform(multi_label_data['labels'])\n",
    "\n",
    "        print(\"Preprocessing complete.\")\n",
    "        return X_bow, X_lda, y, mlb.classes_\n",
    "\n",
    "# Initialize tokenizer and preprocessor\n",
    "stop_words = set(stopwords.words('english')) | {\"reuters\", \"news\"}\n",
    "tokenizer = UnifiedTokenizer(stop_words=stop_words, min_length=4, use_stemming=True)\n",
    "preprocessor = ReutersPreprocessor(tokenizer=tokenizer)\n",
    "X_bow, X_lda, y, class_labels = preprocessor.preprocess()\n",
    "\n",
    "# Combine BoW and LDA features\n",
    "X_combined = np.hstack((X_bow, X_lda))\n",
    "\n",
    "# Save preprocessed data for future use\n",
    "np.save(\"X_combined.npy\", X_combined)\n",
    "np.save(\"y.npy\", y)\n",
    "np.save(\"class_labels.npy\", class_labels)\n",
    "\n",
    "# Model Building\n",
    "model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(X_combined.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(len(class_labels), activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_combined, y,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"best_reuters_model_with_lda.h5\")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_combined, y, verbose=2)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Predict and generate classification report\n",
    "y_pred = model.predict(X_combined)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y, y_pred_binary, target_names=class_labels))\n",
    "\n",
    "overall_accuracy = accuracy_score(y, y_pred_binary)\n",
    "print(f\"\\nOverall Test Accuracy: {overall_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f4e91c-a9a1-4823-941b-d2ae6283052b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
