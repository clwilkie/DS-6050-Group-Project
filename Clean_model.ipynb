{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04b3ef56-f07f-4707-8017-c06ed37affb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/rebeccavannostrand/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rebeccavannostrand/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics:\n",
      "Topic #1:\n",
      "usda production agriculture last corn grain ec sugar wheat tonnes\n",
      "Topic #2:\n",
      "also dollar market agreement told countries japanese japan would trade\n",
      "Topic #3:\n",
      "sales reported expects per gold first share earnings company quarter\n",
      "Topic #4:\n",
      "bid inc would stake stock share group company offer shares\n",
      "Topic #5:\n",
      "market interest billion week money banks fed rates bank rate\n",
      "Topic #6:\n",
      "barrels energy bpd price production opec gas crude prices oil\n",
      "Topic #7:\n",
      "tax would new ltd group last profits francs company billion\n",
      "Topic #8:\n",
      "nine shrs avg oper inc note revs qtr shr cts\n",
      "Topic #9:\n",
      "dealers today money futures one brazil coffee market bank stg\n",
      "Topic #10:\n",
      "deficit march surplus rise december fell rose february january billion\n",
      "Topic #11:\n",
      "state court workers two would port strike south spokesman union\n",
      "Topic #12:\n",
      "rise billion economy bank yen foreign economic government dollar growth\n",
      "Topic #13:\n",
      "share sale common co unit shares stock corp company inc\n",
      "Topic #14:\n",
      "iranian american federal national first iran savings loan gulf bank\n",
      "Topic #15:\n",
      "international prices may buffer new tonnes price cocoa stock nil\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeccavannostrand/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.0910 - loss: 0.1078 - val_accuracy: 0.3946 - val_loss: 0.0333 - learning_rate: 5.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.3203 - loss: 0.0461 - val_accuracy: 0.6269 - val_loss: 0.0134 - learning_rate: 5.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4852 - loss: 0.0161 - val_accuracy: 0.6657 - val_loss: 0.0052 - learning_rate: 5.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5713 - loss: 0.0069 - val_accuracy: 0.6813 - val_loss: 0.0036 - learning_rate: 5.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6317 - loss: 0.0044 - val_accuracy: 0.6987 - val_loss: 0.0030 - learning_rate: 5.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6756 - loss: 0.0034 - val_accuracy: 0.7184 - val_loss: 0.0026 - learning_rate: 5.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7007 - loss: 0.0029 - val_accuracy: 0.7341 - val_loss: 0.0024 - learning_rate: 5.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7184 - loss: 0.0025 - val_accuracy: 0.7520 - val_loss: 0.0022 - learning_rate: 5.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7446 - loss: 0.0022 - val_accuracy: 0.7654 - val_loss: 0.0021 - learning_rate: 5.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7706 - loss: 0.0019 - val_accuracy: 0.7816 - val_loss: 0.0020 - learning_rate: 5.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8008 - loss: 0.0016 - val_accuracy: 0.7920 - val_loss: 0.0019 - learning_rate: 5.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8204 - loss: 0.0014 - val_accuracy: 0.8088 - val_loss: 0.0019 - learning_rate: 5.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8336 - loss: 0.0012 - val_accuracy: 0.8163 - val_loss: 0.0017 - learning_rate: 5.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8531 - loss: 0.0010 - val_accuracy: 0.8250 - val_loss: 0.0016 - learning_rate: 5.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8687 - loss: 8.8592e-04 - val_accuracy: 0.8331 - val_loss: 0.0017 - learning_rate: 5.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8741 - loss: 7.7188e-04 - val_accuracy: 0.8378 - val_loss: 0.0016 - learning_rate: 5.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8783 - loss: 6.9797e-04 - val_accuracy: 0.8384 - val_loss: 0.0016 - learning_rate: 5.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8788 - loss: 6.4623e-04 - val_accuracy: 0.8418 - val_loss: 0.0016 - learning_rate: 5.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8920 - loss: 5.2659e-04 - val_accuracy: 0.8401 - val_loss: 0.0015 - learning_rate: 5.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8907 - loss: 4.8783e-04 - val_accuracy: 0.8430 - val_loss: 0.0015 - learning_rate: 5.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8969 - loss: 4.2408e-04 - val_accuracy: 0.8459 - val_loss: 0.0016 - learning_rate: 5.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8956 - loss: 4.0054e-04 - val_accuracy: 0.8465 - val_loss: 0.0015 - learning_rate: 2.5000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8931 - loss: 3.7497e-04 - val_accuracy: 0.8488 - val_loss: 0.0016 - learning_rate: 2.5000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9022 - loss: 3.4372e-04 - val_accuracy: 0.8494 - val_loss: 0.0016 - learning_rate: 2.5000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8908 - loss: 3.4694e-04 - val_accuracy: 0.8534 - val_loss: 0.0016 - learning_rate: 2.5000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9105 - loss: 3.0128e-04 - val_accuracy: 0.8465 - val_loss: 0.0016 - learning_rate: 2.5000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9007 - loss: 2.7703e-04 - val_accuracy: 0.8488 - val_loss: 0.0016 - learning_rate: 1.2500e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9021 - loss: 3.0821e-04 - val_accuracy: 0.8465 - val_loss: 0.0016 - learning_rate: 1.2500e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9011 - loss: 2.7342e-04 - val_accuracy: 0.8470 - val_loss: 0.0016 - learning_rate: 1.2500e-04\n",
      "68/68 - 0s - 2ms/step - accuracy: 0.8443 - loss: 0.0011\n",
      "Test Loss: 0.0010646687587723136, Test Accuracy: 0.8443002700805664\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95       469\n",
      "           1       1.00      0.14      0.25         7\n",
      "           2       1.00      0.50      0.67         6\n",
      "           3       0.86      0.60      0.71        20\n",
      "           4       1.00      0.40      0.57        15\n",
      "           5       0.00      0.00      0.00         2\n",
      "           6       0.92      0.71      0.80        17\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       1.00      0.75      0.86        28\n",
      "          10       1.00      0.67      0.80         9\n",
      "          11       0.00      0.00      0.00         0\n",
      "          12       0.84      0.51      0.64        53\n",
      "          13       0.83      0.45      0.59        11\n",
      "          14       0.00      0.00      0.00         0\n",
      "          15       0.90      0.45      0.60        20\n",
      "          16       0.00      0.00      0.00         1\n",
      "          17       0.92      0.81      0.86       106\n",
      "          18       0.00      0.00      0.00         0\n",
      "          19       0.90      0.56      0.69        34\n",
      "          20       1.00      1.00      1.00         1\n",
      "          21       0.99      0.96      0.98       787\n",
      "          22       0.00      0.00      0.00         2\n",
      "          23       0.33      0.33      0.33         3\n",
      "          24       1.00      0.59      0.74        34\n",
      "          25       0.96      0.67      0.79        33\n",
      "          26       0.94      0.85      0.89       128\n",
      "          27       0.00      0.00      0.00         1\n",
      "          28       0.00      0.00      0.00         0\n",
      "          29       0.00      0.00      0.00         8\n",
      "          30       1.00      0.50      0.67         6\n",
      "          31       1.00      0.67      0.80         3\n",
      "          32       0.00      0.00      0.00         3\n",
      "          33       0.00      0.00      0.00         0\n",
      "          34       0.88      0.75      0.81       112\n",
      "          35       1.00      0.38      0.55         8\n",
      "          36       1.00      0.75      0.86         4\n",
      "          37       0.00      0.00      0.00         3\n",
      "          38       1.00      0.46      0.63        13\n",
      "          39       0.00      0.00      0.00         1\n",
      "          40       1.00      0.40      0.57         5\n",
      "          41       0.00      0.00      0.00         1\n",
      "          42       0.00      0.00      0.00         0\n",
      "          43       0.90      0.43      0.58        21\n",
      "          44       0.00      0.00      0.00         2\n",
      "          45       1.00      0.17      0.29         6\n",
      "          46       0.88      0.81      0.84       133\n",
      "          47       0.96      0.76      0.85        34\n",
      "          48       0.00      0.00      0.00         2\n",
      "          49       0.90      0.29      0.44        31\n",
      "          50       0.00      0.00      0.00         2\n",
      "          51       0.00      0.00      0.00         0\n",
      "          52       0.00      0.00      0.00         2\n",
      "          53       0.00      0.00      0.00         1\n",
      "          54       0.73      0.44      0.55        25\n",
      "          55       1.00      0.11      0.20         9\n",
      "          56       0.00      0.00      0.00         1\n",
      "          57       1.00      0.22      0.36         9\n",
      "          58       0.00      0.00      0.00         0\n",
      "          59       0.00      0.00      0.00         8\n",
      "          60       1.00      0.67      0.80         3\n",
      "          61       0.00      0.00      0.00         0\n",
      "          62       0.00      0.00      0.00         2\n",
      "          63       0.00      0.00      0.00         3\n",
      "          64       0.00      0.00      0.00         0\n",
      "          65       0.00      0.00      0.00         2\n",
      "          66       0.80      0.62      0.70        13\n",
      "          67       0.00      0.00      0.00         5\n",
      "          68       1.00      0.11      0.19        19\n",
      "          69       1.00      0.75      0.86        12\n",
      "          70       0.00      0.00      0.00         0\n",
      "          71       0.93      0.68      0.79        60\n",
      "          72       1.00      0.25      0.40         4\n",
      "          73       0.50      0.33      0.40         3\n",
      "          74       1.00      0.33      0.50         3\n",
      "          75       0.00      0.00      0.00         1\n",
      "          76       0.78      0.41      0.54        17\n",
      "          77       0.67      0.50      0.57         4\n",
      "          78       1.00      0.89      0.94        35\n",
      "          79       0.00      0.00      0.00         1\n",
      "          80       0.00      0.00      0.00         1\n",
      "          81       1.00      0.33      0.50         3\n",
      "          82       0.00      0.00      0.00         2\n",
      "          83       1.00      0.56      0.71         9\n",
      "          84       0.97      0.61      0.75        93\n",
      "          85       0.93      0.59      0.72        22\n",
      "          86       0.90      0.75      0.82        60\n",
      "          87       1.00      0.25      0.40         8\n",
      "          88       0.75      0.23      0.35        13\n",
      "          89       1.00      0.50      0.67         4\n",
      "\n",
      "   micro avg       0.95      0.78      0.86      2642\n",
      "   macro avg       0.54      0.32      0.38      2642\n",
      "weighted avg       0.93      0.78      0.83      2642\n",
      " samples avg       0.86      0.83      0.84      2642\n",
      "\n",
      "\n",
      "Overall Test Accuracy: 0.7775718257645968\n",
      "Model saved as 'glove_lda_dense_model.h5'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "from nltk.corpus import reuters, stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.initializers import he_normal\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('reuters')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 1: Tokenization and Preprocessing\n",
    "class Tokenizer:\n",
    "    def __init__(self, stop_words=None):\n",
    "        self.stop_words = stop_words or set(stopwords.words('english'))\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
    "        text = re.sub(r\"\\d+\", \" \", text)\n",
    "        tokens = text.split()\n",
    "        return [word for word in tokens if word not in self.stop_words]\n",
    "\n",
    "# Custom stopwords\n",
    "custom_stopwords = set(stopwords.words(\"english\"))\n",
    "custom_stopwords.update({\"mln\", \"lt\", \"pct\", \"dlrs\", \"vs\", \"000\", \"said\", \"year\", \"loss\", \"profit\", \"net\"})\n",
    "\n",
    "# Load Reuters dataset\n",
    "documents = reuters.fileids()\n",
    "texts = [reuters.raw(doc_id) for doc_id in documents]\n",
    "labels = [reuters.categories(doc_id) for doc_id in documents]\n",
    "\n",
    "tokenizer = Tokenizer(stop_words=custom_stopwords)\n",
    "tokenized_texts = [tokenizer.tokenize(text) for text in texts]\n",
    "\n",
    "# Step 2: Convert texts to Bag-of-Words representation\n",
    "vectorizer = CountVectorizer(max_features=5000, stop_words=list(custom_stopwords))\n",
    "X_bow = vectorizer.fit_transform([\" \".join(tokens) for tokens in tokenized_texts])\n",
    "\n",
    "# Step 3: LDA Topic Modeling\n",
    "n_topics = 15\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "X_lda = lda.fit_transform(X_bow)\n",
    "\n",
    "# Display top words in each topic\n",
    "print(\"LDA Topics:\")\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic #{idx + 1}:\")\n",
    "    print(\" \".join([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]]))\n",
    "\n",
    "# Step 4: Load GloVe Embeddings\n",
    "def load_glove_embeddings(glove_path, embedding_dim, vocabulary):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "    \n",
    "    embedding_matrix = np.zeros((len(vocabulary) + 1, embedding_dim))\n",
    "    for word, idx in vocabulary.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "glove_path = \"glove.6B.100d.txt\"  # Update path accordingly\n",
    "embedding_dim = 100\n",
    "vocabulary = {word: idx for idx, word in enumerate(vectorizer.get_feature_names_out())}\n",
    "embedding_matrix = load_glove_embeddings(glove_path, embedding_dim, vocabulary)\n",
    "\n",
    "# Step 5: Combine GloVe Embeddings with LDA Features\n",
    "X_glove = X_bow.toarray()\n",
    "X_combined = np.hstack((X_glove, X_lda))\n",
    "\n",
    "# Step 6: One-Hot Encode Labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(labels)\n",
    "\n",
    "# Step 7: Train/Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 8: Define Focal Loss Function\n",
    "import tensorflow as tf\n",
    "\n",
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.keras.backend.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        pt = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "        loss = -alpha * tf.math.pow(1. - pt, gamma) * tf.math.log(pt)\n",
    "        return tf.reduce_mean(loss)\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# Step 9: Build the Model\n",
    "model = Sequential([\n",
    "    Dense(512, activation=\"relu\", input_shape=(X_train.shape[1],), kernel_initializer=he_normal()),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(256, activation=\"relu\", kernel_initializer=he_normal()),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(y_train.shape[1], activation=\"sigmoid\", kernel_initializer=he_normal())\n",
    "])\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0005),\n",
    "    loss=focal_loss(gamma=2.0, alpha=0.25),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the Model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Predict on Test Set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_binary, zero_division=0))\n",
    "\n",
    "# Overall Accuracy\n",
    "overall_accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(f\"\\nOverall Test Accuracy: {overall_accuracy}\")\n",
    "\n",
    "# Save the Model\n",
    "model.save(\"glove_lda_dense_model.h5\")\n",
    "print(\"Model saved as 'glove_lda_dense_model.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a4f964-e00b-430f-be76-e1f24c08831e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
