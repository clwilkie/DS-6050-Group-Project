{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31c370fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "from nltk.corpus import reuters, stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.initializers import he_normal\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a9f0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b3ef56-f07f-4707-8017-c06ed37affb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\Colin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Colin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('reuters')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 1: Tokenization and Preprocessing\n",
    "class Tokenizer:\n",
    "    def __init__(self, stop_words=None):\n",
    "        self.stop_words = stop_words or set(stopwords.words('english'))\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
    "        text = re.sub(r\"\\d+\", \" \", text)\n",
    "        tokens = text.split()\n",
    "        return [word for word in tokens if word not in self.stop_words]\n",
    "\n",
    "# Custom stopwords\n",
    "custom_stopwords = set(stopwords.words(\"english\"))\n",
    "custom_stopwords.update({\"mln\", \"lt\", \"pct\", \"dlrs\", \"vs\", \"000\", \"said\", \"year\", \"loss\", \"profit\", \"net\"})\n",
    "\n",
    "# Load Reuters dataset\n",
    "documents = reuters.fileids()\n",
    "texts = [reuters.raw(doc_id) for doc_id in documents]\n",
    "labels = [reuters.categories(doc_id) for doc_id in documents]\n",
    "\n",
    "tokenizer = Tokenizer(stop_words=custom_stopwords)\n",
    "tokenized_texts = [tokenizer.tokenize(text) for text in texts]\n",
    "\n",
    "# Step 2: Convert texts to Bag-of-Words representation\n",
    "vectorizer = CountVectorizer(max_features=5000, stop_words=list(custom_stopwords))\n",
    "X_bow = vectorizer.fit_transform([\" \".join(tokens) for tokens in tokenized_texts])\n",
    "\n",
    "# Step 3: LDA Topic Modeling\n",
    "n_topics = 15\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "X_lda = lda.fit_transform(X_bow)\n",
    "\n",
    "# Display top words in each topic\n",
    "print(\"LDA Topics:\")\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic #{idx + 1}:\")\n",
    "    print(\" \".join([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]]))\n",
    "\n",
    "# Step 4: Load GloVe Embeddings\n",
    "def load_glove_embeddings(glove_path, embedding_dim, vocabulary):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "    \n",
    "    embedding_matrix = np.zeros((len(vocabulary) + 1, embedding_dim))\n",
    "    for word, idx in vocabulary.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "glove_path = \"glove.6B.100d.txt\"  # Update path accordingly\n",
    "embedding_dim = 100\n",
    "vocabulary = {word: idx for idx, word in enumerate(vectorizer.get_feature_names_out())}\n",
    "embedding_matrix = load_glove_embeddings(glove_path, embedding_dim, vocabulary)\n",
    "\n",
    "# Step 5: Combine GloVe Embeddings with LDA Features\n",
    "X_glove = X_bow.toarray()\n",
    "X_combined = np.hstack((X_glove, X_lda))\n",
    "\n",
    "# Step 6: One-Hot Encode Labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(labels)\n",
    "\n",
    "# Step 7: Train/Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 8: Define Focal Loss Function\n",
    "import tensorflow as tf\n",
    "\n",
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.keras.backend.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        pt = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "        loss = -alpha * tf.math.pow(1. - pt, gamma) * tf.math.log(pt)\n",
    "        return tf.reduce_mean(loss)\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# Step 9: Build the Model\n",
    "model = Sequential([\n",
    "    Dense(512, activation=\"relu\", input_shape=(X_train.shape[1],), kernel_initializer=he_normal()),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(256, activation=\"relu\", kernel_initializer=he_normal()),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(y_train.shape[1], activation=\"sigmoid\", kernel_initializer=he_normal())\n",
    "])\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0005),\n",
    "    loss=focal_loss(gamma=2.0, alpha=0.25),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the Model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Predict on Test Set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_binary, zero_division=0))\n",
    "\n",
    "# Overall Accuracy\n",
    "overall_accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(f\"\\nOverall Test Accuracy: {overall_accuracy}\")\n",
    "\n",
    "# Save the Model\n",
    "model.save(\"glove_lda_dense_model.h5\")\n",
    "print(\"Model saved as 'glove_lda_dense_model.h5'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
